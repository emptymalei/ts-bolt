{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for ts_bolt","text":""},{"location":"changelog/","title":"ts_bolt Changelog","text":""},{"location":"changelog/#2023-01-01-001","title":"2023-01-01, 0.0.1","text":"<p>Init</p>"},{"location":"references/","title":"Introduction","text":""},{"location":"references/#references","title":"References","text":"<p>In this section, we provide the references for the <code>ts_bolt</code> codebase.</p>"},{"location":"references/cli/","title":"CLI","text":""},{"location":"references/cli/#ts_boltcli","title":"ts_bolt.cli","text":""},{"location":"references/cli/#ts_bolt.cli.download_binary_request","title":"<code>download_binary_request(dataset, local_file)</code>","text":"<p>Download remote content in a binary format</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RawFileDataset</code> <p>a RawFileDataset definition</p> required <code>local_file</code> <code>Path</code> <p>where to write the content to</p> required Source code in <code>ts_bolt/cli.py</code> <pre><code>def download_binary_request(dataset: RawFileDataset, local_file: Path) -&gt; None:\n    \"\"\"Download remote content in a binary format\n\n    :param dataset: a RawFileDataset definition\n    :param local_file: where to write the content to\n    \"\"\"\n\n    r = requests.get(dataset.remote)\n    if r.status_code != 200:\n        logger.error(f\"Can not download {dataset}\")\n    else:\n        with open(local_file, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=128):\n                f.write(chunk)\n</code></pre>"},{"location":"references/cli/#ts_bolt.cli.list","title":"<code>list(name)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>name of datasets to list</p> required Source code in <code>ts_bolt/cli.py</code> <pre><code>@bolt.command()\n@click.option(\n    \"--name\",\n    default=None,\n    type=click.Choice(list(dataset_collections.keys())),  # type: ignore[has-type]\n    help=\"name of dataset to be check\",\n    required=False,\n)\ndef list(name: str) -&gt; None:\n    \"\"\"\n    :param name: name of datasets to list\n    \"\"\"\n    if name is None:\n        click.echo_via_pager(\"\\n\".join(f\"{d}\" for d in dataset_collections))\n    else:\n        click.secho(f\"{name} definition: {dataset_collections[name]}\")\n</code></pre>"},{"location":"references/datamodules/gluonts/","title":"GluonTS","text":""},{"location":"references/datamodules/gluonts/#ts_boltdatamodulesgluonts","title":"ts_bolt.datamodules.gluonts","text":""},{"location":"references/datamodules/gluonts/#ts_bolt.datamodules.gluonts.GluonTSDataLoaderConfig","title":"<code>GluonTSDataLoaderConfig</code>  <code>dataclass</code>","text":"<p>Configs for dataloaders from a gluonts dataset</p> <pre><code>dl_config = GluonTSDataLoaderConfig(\n    batch_size=2,\n    transform=None,\n    collate_fn=None,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>batch size for the PyTorch DataLoader</p> required <code>transform</code> <code>Optional[Callable]</code> <p>transforms of the PyTorch DataLoader, e.g., GluonTSTransformsDefault.</p> required <code>collate_fn</code> <code>Optional[Callable]</code> <p>collate_fn of the PyTorch DataLoader, e.g., gluonts.torch.batchify.batchify</p> required Source code in <code>ts_bolt/datamodules/gluonts.py</code> <pre><code>@dataclass\nclass GluonTSDataLoaderConfig:\n    \"\"\"Configs for dataloaders from a gluonts dataset\n\n\n    ```python\n    dl_config = GluonTSDataLoaderConfig(\n        batch_size=2,\n        transform=None,\n        collate_fn=None,\n    )\n    ```\n\n    :param batch_size: batch size for the PyTorch DataLoader\n    :param transform: transforms of the PyTorch DataLoader, e.g., GluonTSTransformsDefault.\n    :param collate_fn: collate_fn of the PyTorch DataLoader, e.g., gluonts.torch.batchify.batchify\n    \"\"\"\n\n    batch_size: int\n    transform: Optional[Callable]\n    collate_fn: Optional[Callable]\n\n    def __post_init__(self):\n        if self.collate_fn is None:\n            self.collate_fn = batchify\n</code></pre>"},{"location":"references/datamodules/gluonts/#ts_bolt.datamodules.gluonts.GluonTSDataModule","title":"<code>GluonTSDataModule</code>","text":"<p>               Bases: <code>LightningDataModule</code></p> <p>LightningDataModule from a gluonts dataset.</p> <pre><code>from gluonts.dataset.repository.datasets import get_dataset\n\ngluonts_datasets = get_dataset(\"electricity\")\n\ntrain_dl_config = GluonTSDataLoaderConfig(\n    batch_size=2,\n    transform=None,\n    collate_fn=None,\n)\ntest_dl_config = GluonTSDataLoaderConfig(\n    batch_size=10,\n    transform=None,\n    collate_fn=None,\n)\n\ndm = GluonTSDataModule(\n    train_dataset = gluonts_datasets.train,\n    test_dataset = gluonts_datasets.test,\n    train_dataloader_config = train_dl_config,\n    test_dataloader_config = test_dl_config,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>gluonts Dataset for training</p> required <code>train_dataloader_config</code> <code>GluonTSDataLoaderConfig</code> <p>config for train DataLoader</p> required <code>test_dataloader_config</code> <code>GluonTSDataLoaderConfig</code> <p>config for the test DataLoader</p> required Source code in <code>ts_bolt/datamodules/gluonts.py</code> <pre><code>class GluonTSDataModule(pl.LightningDataModule):\n    \"\"\"LightningDataModule from a gluonts dataset.\n\n\n    ```python\n    from gluonts.dataset.repository.datasets import get_dataset\n\n    gluonts_datasets = get_dataset(\"electricity\")\n\n    train_dl_config = GluonTSDataLoaderConfig(\n        batch_size=2,\n        transform=None,\n        collate_fn=None,\n    )\n    test_dl_config = GluonTSDataLoaderConfig(\n        batch_size=10,\n        transform=None,\n        collate_fn=None,\n    )\n\n    dm = GluonTSDataModule(\n        train_dataset = gluonts_datasets.train,\n        test_dataset = gluonts_datasets.test,\n        train_dataloader_config = train_dl_config,\n        test_dataloader_config = test_dl_config,\n    )\n    ```\n\n    :param train_dataset: gluonts Dataset for training\n    :param train_dataset: gluonts Dataset for testing\n    :param train_dataloader_config: config for train DataLoader\n    :param test_dataloader_config: config for the test DataLoader\n    \"\"\"\n\n    def __init__(\n        self,\n        train_dataset: Dataset,\n        test_dataset: Dataset,\n        train_dataloader_config: GluonTSDataLoaderConfig,\n        test_dataloader_config: GluonTSDataLoaderConfig,\n    ):\n        super().__init__()\n        self.train_dataset = train_dataset\n        self.test_dataset = test_dataset\n        self.train_dataloader_config = train_dataloader_config\n        self.test_dataloader_config = test_dataloader_config\n\n    def train_dataloader(self):\n        return DataLoader(\n            dataset=GluonTSDataset(\n                dataset=self.train_dataset,\n                is_train=True,\n                transform=self.train_dataloader_config.transform,\n            ),\n            batch_size=self.train_dataloader_config.batch_size,\n            collate_fn=self.train_dataloader_config.collate_fn,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            dataset=GluonTSDataset(\n                dataset=self.test_dataset,\n                is_train=False,\n                transform=self.test_dataloader_config.transform,\n            ),\n            batch_size=self.test_dataloader_config.batch_size,\n            collate_fn=self.test_dataloader_config.collate_fn,\n        )\n</code></pre>"},{"location":"references/datamodules/gluonts/#ts_bolt.datamodules.gluonts.GluonTSDataset","title":"<code>GluonTSDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>An iter style dataset built from a gluonts dataset</p> <pre><code>from gluonts.dataset.repository.datasets import get_dataset\n\ngluonts_datasets = get_dataset(\"electricity\")\n\ndataset = GluonTSDataset(\n    dataset = gluonts_datasets.train,\n    is_train = True\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>gluonts dataset, e.g., TrainDatasets</p> required <code>is_train</code> <code>bool</code> <p>whether the dataset is for training</p> required <code>transform</code> <code>Optional[Callable]</code> <p>transformations on dataset, e.g., gluonts.transform.InstanceSplitter</p> <code>None</code> Source code in <code>ts_bolt/datamodules/gluonts.py</code> <pre><code>class GluonTSDataset(IterableDataset):\n    \"\"\"An iter style dataset built from a gluonts dataset\n\n    ```python\n    from gluonts.dataset.repository.datasets import get_dataset\n\n    gluonts_datasets = get_dataset(\"electricity\")\n\n    dataset = GluonTSDataset(\n        dataset = gluonts_datasets.train,\n        is_train = True\n    )\n    ```\n\n    :param dataset: gluonts dataset, e.g., TrainDatasets\n    :param is_train: whether the dataset is for training\n    :param transform: transformations on dataset, e.g., gluonts.transform.InstanceSplitter\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset: Dataset,\n        is_train: bool,\n        transform: Optional[Callable] = None,\n        metadata: Optional[Dict[Any, Any]] = None,\n    ):\n        self.metadata = metadata\n        self.dataset = dataset\n        self.is_train = is_train\n\n        self.transform = transform\n        self.transformed_dataset = self._transform_dataset()\n\n    def __iter__(self):\n        for d in self.transformed_dataset:\n            yield d\n\n    def _transform_dataset(self) -&gt; List[Dict[str, Any]]:\n\n        if self.transform:\n            dataset = self.transform(self.dataset, is_train=self.is_train)\n        else:\n            dataset = self.dataset\n\n        return dataset\n</code></pre>"},{"location":"references/datamodules/gluonts/#ts_bolt.datamodules.gluonts.GluonTSTransformsDefault","title":"<code>GluonTSTransformsDefault</code>","text":"<p>               Bases: <code>Transformation</code></p> <p>Default transforms of a gluonts dataset</p> <pre><code>gluonts_transform = GluonTSTransformsDefault(\n    context_length=10,\n    prediction_length=5,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>context_length</code> <code>int</code> <p>the length of history input</p> required <code>prediction_length</code> <code>int</code> <p>the length to be forecasted</p> required Source code in <code>ts_bolt/datamodules/gluonts.py</code> <pre><code>class GluonTSTransformsDefault(Transformation):\n    \"\"\"Default transforms of a gluonts dataset\n\n    ```python\n    gluonts_transform = GluonTSTransformsDefault(\n        context_length=10,\n        prediction_length=5,\n    )\n    ```\n\n    :param context_length: the length of history input\n    :param prediction_length: the length to be forecasted\n    \"\"\"\n\n    def __init__(self, context_length: int, prediction_length: int):\n        self.context_length = context_length\n        self.prediction_length = prediction_length\n\n    def __call__(self, data_it: Iterable[Dict[str, Any]], is_train: bool):\n        mask_unobserved = AddObservedValuesIndicator(\n            target_field=FieldName.TARGET,\n            output_field=FieldName.OBSERVED_VALUES,\n        )\n\n        training_splitter = InstanceSplitter(\n            target_field=FieldName.TARGET,\n            is_pad_field=FieldName.IS_PAD,\n            start_field=FieldName.START,\n            forecast_start_field=FieldName.FORECAST_START,\n            instance_sampler=ExpectedNumInstanceSampler(\n                num_instances=1,\n                min_future=self.prediction_length,\n            ),\n            past_length=self.context_length,\n            future_length=self.prediction_length,\n            time_series_fields=[FieldName.OBSERVED_VALUES],\n        )\n\n        transforms = mask_unobserved + training_splitter\n\n        return transforms(data_it=data_it, is_train=is_train)\n</code></pre>"},{"location":"references/datamodules/pandas/","title":"Pandas","text":""},{"location":"references/datamodules/pandas/#ts_boltdatamodulespandas","title":"ts_bolt.datamodules.pandas","text":""},{"location":"references/datamodules/pandas/#ts_bolt.datamodules.pandas.DataFrameDataset","title":"<code>DataFrameDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset from a pandas dataframe.</p> <p>For a given pandas dataframe, this generates a pytorch compatible dataset by sliding in time dimension.</p> <pre><code>ds = DataFrameDataset(\n    dataframe=df, history_length=10, horizon=2\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>input dataframe with a DatetimeIndex.</p> required <code>history_length</code> <code>int</code> <p>length of input X in time dimension in the final Dataset class.</p> required <code>horizon</code> <code>int</code> <p>number of steps to be forecasted.</p> required <code>gap</code> <code>int</code> <p>gap between input history and prediction</p> <code>0</code> Source code in <code>ts_bolt/datamodules/pandas.py</code> <pre><code>class DataFrameDataset(Dataset):\n    \"\"\"A dataset from a pandas dataframe.\n\n    For a given pandas dataframe, this generates a pytorch\n    compatible dataset by sliding in time dimension.\n\n    ```python\n    ds = DataFrameDataset(\n        dataframe=df, history_length=10, horizon=2\n    )\n    ```\n\n    :param dataframe: input dataframe with a DatetimeIndex.\n    :param history_length: length of input X in time dimension\n        in the final Dataset class.\n    :param horizon: number of steps to be forecasted.\n    :param gap: gap between input history and prediction\n    \"\"\"\n\n    def __init__(\n        self, dataframe: pd.DataFrame, history_length: int, horizon: int, gap: int = 0\n    ):\n        super().__init__()\n        self.dataframe = dataframe\n        self.history_length = history_length\n        self.horzion = horizon\n        self.gap = gap\n        self.dataframe_rows = len(self.dataframe)\n        self.length = (\n            self.dataframe_rows - self.history_length - self.horzion - self.gap + 1\n        )\n\n    def moving_slicing(self, idx: int, gap: int = 0) -&gt; Tuple[np.ndarray, np.ndarray]:\n        x, y = (\n            self.dataframe[idx : self.history_length + idx].values,\n            self.dataframe[\n                self.history_length\n                + idx\n                + gap : self.history_length\n                + self.horzion\n                + idx\n                + gap\n            ].values,\n        )\n        return x, y\n\n    def _validate_dataframe(self) -&gt; None:\n        \"\"\"Validate the input dataframe.\n\n        - We require the dataframe index to be DatetimeIndex.\n        - This dataset is null aversion.\n        - Dataframe index should be sorted.\n        \"\"\"\n\n        if not isinstance(\n            self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n        ):\n            raise TypeError(\n                \"Type of the dataframe index is not DatetimeIndex\"\n                f\": {type(self.dataframe.index)}\"\n            )\n\n        has_na = self.dataframe.isnull().values.any()\n\n        if has_na:\n            logger.warning(\"Dataframe has null\")\n\n        has_index_sorted = self.dataframe.index.equals(\n            self.dataframe.index.sort_values()\n        )\n\n        if not has_index_sorted:\n            logger.warning(\"Dataframe index is not sorted\")\n\n    def __getitem__(self, idx: int) -&gt; Tuple[np.ndarray, np.ndarray]:\n        if isinstance(idx, slice):\n            if (idx.start &lt; 0) or (idx.stop &gt;= self.length):\n                raise IndexError(f\"Slice out of range: {idx}\")\n            step = idx.step if idx.step is not None else 1\n            return [\n                self.moving_slicing(i, self.gap)\n                for i in range(idx.start, idx.stop, step)\n            ]\n        else:\n            if idx &gt;= self.length:\n                raise IndexError(\"End of dataset\")\n            return self.moving_slicing(idx, self.gap)\n\n    def __len__(self) -&gt; int:\n        return self.length\n</code></pre>"},{"location":"references/datamodules/pandas/#ts_bolt.datamodules.pandas.DataFrameDataset._validate_dataframe","title":"<code>_validate_dataframe()</code>","text":"<p>Validate the input dataframe.</p> <ul> <li>We require the dataframe index to be DatetimeIndex.</li> <li>This dataset is null aversion.</li> <li>Dataframe index should be sorted.</li> </ul> Source code in <code>ts_bolt/datamodules/pandas.py</code> <pre><code>def _validate_dataframe(self) -&gt; None:\n    \"\"\"Validate the input dataframe.\n\n    - We require the dataframe index to be DatetimeIndex.\n    - This dataset is null aversion.\n    - Dataframe index should be sorted.\n    \"\"\"\n\n    if not isinstance(\n        self.dataframe.index, pd.core.indexes.datetimes.DatetimeIndex\n    ):\n        raise TypeError(\n            \"Type of the dataframe index is not DatetimeIndex\"\n            f\": {type(self.dataframe.index)}\"\n        )\n\n    has_na = self.dataframe.isnull().values.any()\n\n    if has_na:\n        logger.warning(\"Dataframe has null\")\n\n    has_index_sorted = self.dataframe.index.equals(\n        self.dataframe.index.sort_values()\n    )\n\n    if not has_index_sorted:\n        logger.warning(\"Dataframe index is not sorted\")\n</code></pre>"},{"location":"references/datasets/collections/","title":"Collections","text":""},{"location":"references/datasets/collections/#ts_boltdatasetscollections","title":"ts_bolt.datasets.collections","text":""},{"location":"references/datasets/collections/#ts_bolt.datasets.collections.DownloaderDataset","title":"<code>DownloaderDataset</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dataset that is downloaded by a downloader function.</p> Source code in <code>ts_bolt/datasets/collections.py</code> <pre><code>class DownloaderDataset(BaseModel):\n    \"\"\"Dataset that is downloaded by a downloader function.\"\"\"\n\n    name: str\n    downloader: BaseDownloader\n    documentation: str\n    file_name: str\n    description: str\n</code></pre>"},{"location":"references/datasets/collections/#ts_bolt.datasets.collections.NamedDatasets","title":"<code>NamedDatasets</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>NamedDatasets is a collection of datasets that are used in the ts_bolt library.</p> Source code in <code>ts_bolt/datasets/collections.py</code> <pre><code>class NamedDatasets(BaseModel):\n    \"\"\"\n    NamedDatasets is a collection of datasets\n    that are used in the ts_bolt library.\n    \"\"\"\n\n    lai_exchange_rate: str = \"lai_exchange_rate\"\n    lai_electricity: str = \"lai_electricity\"\n    lai_solar_al: str = \"lai_solar_al\"\n    lai_traffic: str = \"lai_traffic\"\n    ecb_exchange_rate: str = \"ecb_exchange_rate\"\n    mbohlkeschneider_electricity_nips: str = \"mbohlkeschneider_electricity_nips\"\n    mbohlkeschneider_exchange_rate_nips: str = \"mbohlkeschneider_exchange_rate_nips\"\n    mbohlkeschneider_solar_nips: str = \"mbohlkeschneider_solar_nips\"\n    mbohlkeschneider_wiki_rolling_nips: str = \"mbohlkeschneider_wiki_rolling_nips\"\n    mbohlkeschneider_traffic_nips: str = \"mbohlkeschneider_traffic_nips\"\n    mbohlkeschneider_taxi_30min: str = \"mbohlkeschneider_taxi_30min\"\n</code></pre>"},{"location":"references/datasets/collections/#ts_bolt.datasets.collections.RawFileDataset","title":"<code>RawFileDataset</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>A model defining raw file datasets.</p> Source code in <code>ts_bolt/datasets/collections.py</code> <pre><code>class RawFileDataset(BaseModel):\n    \"\"\"A model defining raw file datasets.\"\"\"\n\n    name: str\n    remote: str\n    documentation: str\n    file_name: str\n    description: str\n</code></pre>"},{"location":"references/datasets/downloaders/base/","title":"Base","text":""},{"location":"references/datasets/downloaders/base/#ts_boltdatasetsdownloadersbase","title":"ts_bolt.datasets.downloaders.base","text":""},{"location":"references/datasets/downloaders/base/#ts_bolt.datasets.downloaders.base.BaseDownloader","title":"<code>BaseDownloader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for a data file downloader.</p> Source code in <code>ts_bolt/datasets/downloaders/base.py</code> <pre><code>class BaseDownloader(ABC):\n    \"\"\"Base class for a data file downloader.\"\"\"\n\n    @abstractmethod\n    def __init__(self, *args: Any, **kwargs: Any):\n        pass\n\n    @abstractmethod\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        pass\n</code></pre>"},{"location":"references/datasets/downloaders/ecb_exchange_rate/","title":"Exchange Rate","text":""},{"location":"references/datasets/downloaders/ecb_exchange_rate/#ts_boltdatasetsdownloadersecb_exchange_rate","title":"ts_bolt.datasets.downloaders.ecb_exchange_rate","text":""},{"location":"references/datasets/downloaders/ecb_exchange_rate/#ts_bolt.datasets.downloaders.ecb_exchange_rate.ECBExchangeRateDownloader","title":"<code>ECBExchangeRateDownloader</code>","text":"<p>               Bases: <code>BaseDownloader</code></p> <p>Download the ECB Exchange Rate dataset and explore some basic features of the dataset.</p> <ul> <li>Dataset Website: https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html</li> <li>Dataset Download Link: https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip</li> </ul> Source code in <code>ts_bolt/datasets/downloaders/ecb_exchange_rate.py</code> <pre><code>class ECBExchangeRateDownloader(BaseDownloader):\n    \"\"\"Download the ECB Exchange Rate dataset and explore some basic features of the dataset.\n\n    - Dataset Website: https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\n    - Dataset Download Link: https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        self.url = \"https://www.ecb.europa.eu\" \"/stats/eurofxref/eurofxref-hist.zip\"\n\n    def __call__(self, target: Path) -&gt; None:\n        \"\"\"Download the ECB Exchange Rate dataset and explore some basic features of the dataset.\n        - Dataset Website: https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\n        - Dataset Download Link: https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip\n        \"\"\"\n        if target.exists():\n            logger.warning(f\"dataset exists in {target}, download skipped\")\n        else:\n            df = pd.read_csv(\n                self.url,\n                compression=\"zip\",\n            )\n\n            df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date\n\n            # There are some empty columns.\n            columns = [col for col in df.columns if not df[col].isna().all()]\n\n            df = df[columns]\n\n            df.to_csv(target, index=False)\n</code></pre>"},{"location":"references/datasets/downloaders/ecb_exchange_rate/#ts_bolt.datasets.downloaders.ecb_exchange_rate.ECBExchangeRateDownloader.__call__","title":"<code>__call__(target)</code>","text":"<p>Download the ECB Exchange Rate dataset and explore some basic features of the dataset. - Dataset Website: https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html - Dataset Download Link: https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip</p> Source code in <code>ts_bolt/datasets/downloaders/ecb_exchange_rate.py</code> <pre><code>def __call__(self, target: Path) -&gt; None:\n    \"\"\"Download the ECB Exchange Rate dataset and explore some basic features of the dataset.\n    - Dataset Website: https://www.ecb.europa.eu/stats/policy_and_exchange_rates/euro_reference_exchange_rates/html/index.en.html\n    - Dataset Download Link: https://www.ecb.europa.eu/stats/eurofxref/eurofxref-hist.zip\n    \"\"\"\n    if target.exists():\n        logger.warning(f\"dataset exists in {target}, download skipped\")\n    else:\n        df = pd.read_csv(\n            self.url,\n            compression=\"zip\",\n        )\n\n        df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date\n\n        # There are some empty columns.\n        columns = [col for col in df.columns if not df[col].isna().all()]\n\n        df = df[columns]\n\n        df.to_csv(target, index=False)\n</code></pre>"},{"location":"references/evaluation/evaluator/","title":"Evaluator","text":""},{"location":"references/evaluation/evaluator/#ts_boltevaluationevaluator","title":"ts_bolt.evaluation.evaluator","text":""},{"location":"references/evaluation/evaluator/#ts_bolt.evaluation.evaluator.Evaluator","title":"<code>Evaluator</code>","text":"<p>Evaluate the predictions</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>which prediction step to be evaluated.</p> <code>0</code> <code>gap</code> <code>int</code> <p>gap between input history and target/prediction.</p> <code>0</code> Source code in <code>ts_bolt/evaluation/evaluator.py</code> <pre><code>class Evaluator:\n    \"\"\"Evaluate the predictions\n\n    :param step: which prediction step to be evaluated.\n    :param gap: gap between input history and target/prediction.\n    \"\"\"\n\n    def __init__(self, step: int = 0, gap: int = 0):\n        self.step = step\n        self.gap = gap\n\n    @staticmethod\n    def get_one_history(\n        predictions: Sequence[Sequence], idx: int, batch_idx: int = 0\n    ) -&gt; torch.Tensor:\n        return predictions[batch_idx][0][idx, ...]\n\n    @staticmethod\n    def get_one_pred(predictions: List, idx: int, batch_idx: int = 0) -&gt; torch.Tensor:\n        return predictions[batch_idx][1][idx, ...]\n\n    @staticmethod\n    def get_y(predictions: List, step: int) -&gt; List[torch.Tensor]:\n        return [i[1][..., step] for i in predictions]\n\n    def y(self, predictions: List, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y(predictions, self.step)[batch_idx].detach()\n\n    @staticmethod\n    def get_y_true(dataloader: DataLoader, step: int) -&gt; list[torch.Tensor]:\n        return [i[1].squeeze(-1)[..., step] for i in dataloader]\n\n    def y_true(self, dataloader: DataLoader, batch_idx: int = 0) -&gt; torch.Tensor:\n        return self.get_y_true(dataloader, step=self.step)[batch_idx].detach()\n\n    def get_one_sample(\n        self, predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        return (\n            self.get_one_history(predictions, idx, batch_idx),\n            self.get_one_pred(predictions, idx, batch_idx),\n        )\n\n    def plot_one_sample(\n        self, ax: mpl.axes.Axes, predictions: List, idx: int, batch_idx: int = 0\n    ) -&gt; None:\n        history, pred = self.get_one_sample(predictions, idx, batch_idx)\n\n        x_raw = np.arange(len(history) + len(pred) + self.gap)\n        x_history = x_raw[: len(history)]\n        x_pred = x_raw[len(history) + self.gap :]\n        x = np.concatenate([x_history, x_pred])\n\n        y = np.concatenate([history, pred])\n\n        ax.plot(x, y, marker=\".\", label=f\"input ({idx})\")\n\n        ax.axvspan(x_pred[0], x_pred[-1], color=\"orange\", alpha=0.1)\n\n    @property\n    def metric_collection(self) -&gt; MetricCollection:\n        return MetricCollection(\n            MeanAbsoluteError(),\n            MeanAbsolutePercentageError(),\n            MeanSquaredError(),\n            SymmetricMeanAbsolutePercentageError(),\n        )\n\n    @staticmethod\n    def metric_dataframe(metrics: Dict) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            [{k: float(v) for k, v in metrics.items()}], index=[\"values\"]\n        ).T\n\n    def metrics(\n        self, predictions: List, dataloader: DataLoader, batch_idx: int = 0\n    ) -&gt; pd.DataFrame:\n        truths = self.y_true(dataloader)\n        preds = self.y(predictions, batch_idx=batch_idx)\n\n        return self.metric_dataframe(self.metric_collection(preds, truths))\n</code></pre>"},{"location":"references/naive_forecasters/last_observation/","title":"Last Observation","text":""},{"location":"references/naive_forecasters/last_observation/#ts_boltnaive_forecasterslast_observation","title":"ts_bolt.naive_forecasters.last_observation","text":""},{"location":"references/naive_forecasters/last_observation/#ts_bolt.naive_forecasters.last_observation.LastObservationForecaster","title":"<code>LastObservationForecaster</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>Spits out the forecasts using the last observation.</p> <p>Parameters:</p> Name Type Description Default <code>horizon</code> <code>int</code> <p>horizon of the forecast.</p> required Source code in <code>ts_bolt/naive_forecasters/last_observation.py</code> <pre><code>class LastObservationForecaster(L.LightningModule):\n    \"\"\"Spits out the forecasts using the last observation.\n\n    :param horizon: horizon of the forecast.\n    \"\"\"\n\n    def __init__(self, horizon: int):\n        super().__init__()\n        self.horizon = horizon\n\n    def _last_observation(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return x[..., -1:, :]\n\n    def predict_step(\n        self, batch: Sequence[torch.Tensor], batch_idx: int\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x, y = batch\n\n        y_hat = self._last_observation(x)\n\n        y_hat = y_hat.repeat(1, self.horizon, 1)\n\n        return x.squeeze(-1), y_hat.squeeze(-1)\n\n    def forward(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        x = x.type(self.dtype)\n        return (\n            x.squeeze(-1),\n            self._last_observation(x).repeat(1, self.horizon, 1).squeeze(-1),\n        )\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>Tutorials for <code>ts_bolt</code>.</p>"},{"location":"tutorials/command-line/","title":"Command Line Tool","text":"<p>Use the command line tool to find and download data.</p> <pre><code>bolt list\n</code></pre> <pre><code>bolt download --name=lai_electricity --target=datasets\n</code></pre>"},{"location":"tutorials/pandas-dataframe/","title":"Pandas DataFrame as Dataset","text":"<p>In this tutorial, we show a few examples using pandas dataframe as dataset.</p>"},{"location":"tutorials/pandas-dataframe/#ts_boltdatamodulespandasdataframedataset","title":"<code>ts_bolt.datamodules.pandas.DataFrameDataset</code>","text":"<p><code>ts_bolt.datamodules.pandas.DataFrameDataset</code> takes a pandas dataframe and converts it to a PyTorch dataset.</p> <pre><code>import pandas as pd\n\nfrom ts_bolt.datamodules.pandas import DataFrameDataset\n\ndates = pd.date_range(\"2021-01-01\", \"2021-04-01\", freq=\"D\")\n\ndf_a = pd.DataFrame(\n    {\n        \"date\": dates,\n        \"target\": range(len(dates)),\n        \"item_id\": [\"A\"] * len(dates)\n    }\n)\ndf_b = pd.DataFrame(\n    {\n        \"date\": dates,\n        \"target\": range(len(dates)),\n        \"item_id\": [\"B\"] * len(dates)\n    }\n)\n\ndf_long = pd.concat([df_a, df_b])\n\ndf_wide = (\n    df_long\n    .pivot(index=\"date\", columns=\"item_id\", values=\"target\")\n)\n</code></pre> <p>With this wide dataframe, we can construct a PyTorch dataset</p> <pre><code>dfds = DataFrameDataset(dataframe=df_wide, context_length=3, horizon=2)\n\nnext(iter(dfds))\n</code></pre>"},{"location":"tutorials/pandas-dataframe/#using-gluonts-pandasdataset","title":"Using GluonTS PandasDataset","text":"<p><code>ts_bolt.datamodules.gluonts</code> provides a generic connection between gluonts datasets and pytorch dataloader.</p> <pre><code>gluonts_pds = PandasDataset.from_long_dataframe(\n    pandas_dataframe, target=\"target\", item_id=\"item_id\"\n)\n\nds = GluonTSDataset(dataset=gluonts_pds, is_train=True, transform=gluonts_transform)\n</code></pre>"},{"location":"tutorials/pandas-dataframe/#dataloader","title":"DataLoader","text":"<p>Once we obtained the dataset, a dataloader can be constructed the PyTorch way.</p> <pre><code>dl = DataLoader(ds, batch_size=2, collate_fn=lambda data: data)\nnext(iter(dl))\n</code></pre>"}]}